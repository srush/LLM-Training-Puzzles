{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d8204266",
      "metadata": {
        "id": "d8204266"
      },
      "source": [
        "# LLM Training Puzzles\n",
        "\n",
        "by Sasha Rush ([@srush_nlp](https://twitter.com/srush_nlp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65be14b",
      "metadata": {
        "id": "c65be14b"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Uncomment to run in Colab\n",
        "!pip install -qqq git+https://github.com/chalk-diagrams/chalk asyncio\n",
        "!wget https://raw.githubusercontent.com/srush/LLM-Training-Puzzles/main/lib.py https://raw.githubusercontent.com/srush/LLM-Training-Puzzles/main/drawing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d583c979",
      "metadata": {
        "id": "d583c979"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from lib import Model, Dist, WeightGrad\n",
        "from drawing import draw, draw_group\n",
        "from chalk import vcat\n",
        "import asyncio\n",
        "import chalk\n",
        "chalk.set_svg_height(400)\n",
        "chalk.set_svg_draw_height(600)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc24dadb",
      "metadata": {
        "id": "cc24dadb"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "The goal of these puzzles is to learn about distributed training of LLMs. However, we will be primarily concerned with a speed and memory efficiency of completing a single update of the models. To make things simpler, we will abstract away from the standard tensor-based transformer model, and just consider a state-less representation of each of the components of a multi-layer neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad71f90c",
      "metadata": {
        "id": "ad71f90c"
      },
      "outputs": [],
      "source": [
        "model = Model(layers=2, batches=4)\n",
        "weights, opt_states, activations, grad_activations, grad_weights = model.storage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61cf6388",
      "metadata": {
        "id": "61cf6388"
      },
      "source": [
        "Our library has 5 parts:\n",
        "\n",
        "* Weights\n",
        "* Optimizer States - Values needed to update the weights\n",
        "* Activations - The internal values computed on the forward pass\n",
        "* Grad Activations - The gradients of the loss wrt to activations, needed for backward pass\n",
        "* Grad Weights - The gradients of the loss wrt to weights, needed for updates\n",
        "\n",
        "For these puzzles, you are *not allowed* to have local variables. You need to store each of these in the dictionary corresponding to its type.                      \n",
        "\n",
        "We begin by tracing the lifecycle of a single model update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0335a17b",
      "metadata": {
        "id": "0335a17b"
      },
      "outputs": [],
      "source": [
        "# Get the input activations to the model for batches 2, 3\n",
        "activations[0] = model.get_activation(batches=[2, 3])\n",
        "activations[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962ac1d8",
      "metadata": {
        "id": "962ac1d8"
      },
      "outputs": [],
      "source": [
        "# Load the weights (random) for layers 0 and 1\n",
        "for i in range(model.LAYERS):\n",
        "    weights[i], opt_states[i] = model.load_weights(i)\n",
        "weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a83439",
      "metadata": {
        "id": "f7a83439"
      },
      "outputs": [],
      "source": [
        "# Activations can be moved forward a layer if you have the weights.\n",
        "activations[1] = model.forward(layer=0, inp=activations[0], weight=weights[0])\n",
        "activations[2] = model.forward(layer=1, inp=activations[1], weight=weights[1])\n",
        "activations[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d904e3",
      "metadata": {
        "id": "81d904e3"
      },
      "outputs": [],
      "source": [
        "# Draw all the current activations in memory.\n",
        "draw_group(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e8769a",
      "metadata": {
        "id": "80e8769a"
      },
      "outputs": [],
      "source": [
        "# At the last layer, we can convert an activation to a grad activation by calling `loss`\n",
        "grad_activations[model.LAYERS] = model.loss(activations[model.LAYERS])\n",
        "grad_activations[model.LAYERS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3299042",
      "metadata": {
        "id": "b3299042"
      },
      "outputs": [],
      "source": [
        "# Calling `backward` requires the forward activation, the backward grad activation, and the weights.\n",
        "# It returns the grad weights and the backward activation.\n",
        "grad_weights[1], grad_activations[1] = model.backward(1, activations[1], grad_activations[2], weights[1])\n",
        "grad_weights[0], grad_activations[0] = model.backward(0, activations[0], grad_activations[1], weights[0])\n",
        "grad_activations[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d82618",
      "metadata": {
        "id": "33d82618"
      },
      "outputs": [],
      "source": [
        "# We can use delete to remove any memory that is not longer needed.\n",
        "print(\"Before memory:\", model.memory())\n",
        "del grad_activations[1]\n",
        "print(\"After memory:\", model.memory())\n",
        "model.status()\n",
        "draw_group(grad_activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48969f0",
      "metadata": {
        "id": "f48969f0"
      },
      "outputs": [],
      "source": [
        "# Grad weights keep track of which batches they are for. Here we only have the grad weights for batches 2 and 3.\n",
        "draw_group(grad_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d469f7",
      "metadata": {
        "id": "d3d469f7"
      },
      "outputs": [],
      "source": [
        "# If we try to update with the grad weights we will get an error.\n",
        "try:\n",
        "    model.update(0, weight_grad=grad_weights[0], weight=weights[0], opt_state=opt_states[0])\n",
        "except AssertionError as e:\n",
        "    print(\"Error! Only have batches\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55046513",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "55046513"
      },
      "outputs": [],
      "source": [
        "# For this example, we can cheat. Pretend we had the other gradients we needed.\n",
        "grad_weights[0, 0] = model.fake_grad(0, [0,1])\n",
        "grad_weights[1, 0] = model.fake_grad(1, [0,1])\n",
        "grad_weights[0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5caee88",
      "metadata": {
        "id": "f5caee88"
      },
      "outputs": [],
      "source": [
        "# Summing together grad_weights gives the full gradient.\n",
        "grad_weights[0] = grad_weights[0] + grad_weights[0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe2165e",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "dfe2165e"
      },
      "outputs": [],
      "source": [
        "# Now we can call update to the get the new weights and opt_state.\n",
        "weights[0], opt_states[0] = model.update(0, weight_grad=grad_weights[0], weight=weights[0],\n",
        "                                         opt_state=opt_states[0])\n",
        "\n",
        "# WARNING: You need to set all variables. Otherwise they are not counted towards memory.\n",
        "grad_weights[1] = grad_weights[1] + grad_weights[1, 0]\n",
        "weights[1], opt_states[1] = model.update(1, weight_grad=grad_weights[1],\n",
        "                                         weight=weights[1], opt_state=opt_states[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9582594",
      "metadata": {
        "id": "f9582594"
      },
      "outputs": [],
      "source": [
        "# We can complete the tests by setting these as the final weights and calling check.\n",
        "model.set_final_weight(0, weights[0])\n",
        "model.set_final_weight(1, weights[1])\n",
        "Model.check([model])\n",
        "draw_group(model.final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2f0a3b",
      "metadata": {
        "id": "7b2f0a3b"
      },
      "outputs": [],
      "source": [
        "# We can view the final outcome of the system as a diagram.\n",
        "# This show the forward and backward passes (numbers of batches) and the updates.\n",
        "# The lines on the bottom show the memory that is used at each time step.\n",
        "draw([model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aa8d66",
      "metadata": {
        "id": "01aa8d66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "26e5ea60",
      "metadata": {
        "id": "26e5ea60"
      },
      "source": [
        "### Puzzle 0 - Standard Training\n",
        "\n",
        "Write a standard (non-distributed) training loop that acts on all the batches and loads all the weights. It should just run forward, loss, backward, and update. Aim for the least amount of max memory used.\n",
        "\n",
        "* Target Time:  16.5 steps\n",
        "* Target Memory: 2621440"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04dbf7ea",
      "metadata": {
        "id": "04dbf7ea"
      },
      "outputs": [],
      "source": [
        "def basic(model: Model) -> Model:\n",
        "    # Storage on device.\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "\n",
        "    # Load in the full weights\n",
        "    for l in range(model.LAYERS):\n",
        "        weights[l], opt_states[l] = model.load_weights(l)\n",
        "\n",
        "    # Load the input layer activations\n",
        "    activations[0] = model.get_activation(range(model.BATCHES))\n",
        "\n",
        "    assert False, 'TODO: Implement me'\n",
        "\n",
        "    for l in range(model.LAYERS):\n",
        "        model.set_final_weight(l, weights[l])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3632f24",
      "metadata": {
        "id": "e3632f24"
      },
      "outputs": [],
      "source": [
        "out = basic(Model(layers=2, batches=4, rank=0, dist=Dist(1)))\n",
        "draw_group(out.final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c87306",
      "metadata": {
        "id": "21c87306"
      },
      "outputs": [],
      "source": [
        "draw([out])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e71f5a",
      "metadata": {
        "id": "74e71f5a"
      },
      "outputs": [],
      "source": [
        "Model.check([out])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25bbcaf",
      "metadata": {
        "id": "d25bbcaf"
      },
      "source": [
        "### Puzzle 1 - Gradient Accumulation\n",
        "\n",
        "For this puzzle, the goal is to reduce max memory usage. To do so you are going to run on each batch individually instead of all together.\n",
        "\n",
        "Write a function with four parts. First run on batches {0} and then {1} etc. Sum the grad weights and then update.\n",
        "\n",
        "* Target Time:  16.5 steps\n",
        "* Target Memory: 1966080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c870ae6",
      "metadata": {
        "id": "4c870ae6"
      },
      "outputs": [],
      "source": [
        "def grad_accum(model: Model) -> Model:\n",
        "    # Storage on device.\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "\n",
        "    # Load in the full weights\n",
        "    for l in range(model.LAYERS):\n",
        "        weights[l], opt_states[l] = model.load_weights(l)\n",
        "\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in range(model.LAYERS):\n",
        "        model.set_final_weight(l, weights[l])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672563cf",
      "metadata": {
        "id": "672563cf"
      },
      "outputs": [],
      "source": [
        "out = grad_accum(Model(layers=2, batches=4, rank=0, dist=Dist(1)))\n",
        "draw_group(out.final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77bb31d7",
      "metadata": {
        "id": "77bb31d7"
      },
      "outputs": [],
      "source": [
        "draw([out])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cf8167",
      "metadata": {
        "id": "28cf8167"
      },
      "outputs": [],
      "source": [
        "Model.check([out])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfb345c",
      "metadata": {
        "id": "ccfb345c"
      },
      "source": [
        "## Communications: AllReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17383602",
      "metadata": {
        "id": "17383602"
      },
      "source": [
        "When working with multiple GPUs we need to have communication.\n",
        "The primary communication primitives for GPUs are implemented in NCCL.\n",
        "\n",
        "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html\n",
        "\n",
        "We are not going to use these directly, but simulate them using Python and asyncio.\n",
        "\n",
        "The first operation is AllReduce. We will have 4 GPUs (ranks=4) and use them each to compute a batch of weight grads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7bb767",
      "metadata": {
        "id": "1f7bb767"
      },
      "outputs": [],
      "source": [
        "ranks = 4\n",
        "weight_grads = [WeightGrad(0, 1, {i}, ranks) for i in range(ranks)]\n",
        "weight_grads[0] + weight_grads[1] + weight_grads[2] + weight_grads[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0232db",
      "metadata": {
        "id": "ef0232db"
      },
      "outputs": [],
      "source": [
        "# Simple asynchronous function that calls allreduce to sum the weight grads at layer 0\n",
        "async def myfunc(model: Model) -> WeightGrad:\n",
        "    return await model.allreduce(weight_grads[model.rank], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a91ae4",
      "metadata": {
        "id": "43a91ae4"
      },
      "outputs": [],
      "source": [
        "# This code uses asyncio to run the above function on 4 \"GPUs\" .\n",
        "dist = Dist(ranks)\n",
        "out_weight_grads = await asyncio.gather(*[\n",
        "    myfunc(Model(layers=1, batches=1, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "out_weight_grads[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d4ee90",
      "metadata": {
        "id": "73d4ee90"
      },
      "source": [
        "Note: When running communication operations like AllReduce on a GPU, the communication happens in parallel to the computation on that GPU. That means the API for AllReduce does not block, and allows the model to continue running while waiting for this command to run. This means it is beneficial to run AllReduce (and other communication) as early as possible so that other compute can be run during the reduction.\n",
        "\n",
        "We will ignore this in these puzzles and represent communication as happening efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1020523e",
      "metadata": {
        "id": "1020523e"
      },
      "source": [
        "### Puzzle 2 - Distributed Data Parallel\n",
        "\n",
        "Write a function with four parts. First run on batches {0} and then {1} etc. Sum the grad weights and then update. The main benefit of this approach is compute efficiency over gradient accumulation.\n",
        "\n",
        "* Total Steps: 5.1\n",
        "* Total Memory: 1835008"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f492668c",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "f492668c"
      },
      "outputs": [],
      "source": [
        "async def ddp(model: Model) -> Model:\n",
        "    # Storage on device.\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "    # Load all the activations\n",
        "    model.activations[0] = model.get_activation([model.rank])\n",
        "\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in range(model.LAYERS):\n",
        "        model.set_final_weight(l, weights[l])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6ed44f7",
      "metadata": {
        "id": "d6ed44f7"
      },
      "outputs": [],
      "source": [
        "dist = Dist(ranks)\n",
        "out = await asyncio.gather(*[\n",
        "    ddp(Model(layers=2, batches=ranks, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "draw_group(out[0].final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89411fad",
      "metadata": {
        "id": "89411fad"
      },
      "outputs": [],
      "source": [
        "draw(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df8166f",
      "metadata": {
        "id": "2df8166f"
      },
      "outputs": [],
      "source": [
        "Model.check(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3df405",
      "metadata": {
        "id": "8c3df405"
      },
      "source": [
        "## Communication: AllGather / Sharding\n",
        "\n",
        "Our next primitive is AllGather. This allows us to communicate \"shards\" of an object stored on different GPUs to all the GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b4bad85",
      "metadata": {
        "id": "5b4bad85"
      },
      "outputs": [],
      "source": [
        "# Load only part of a weights.\n",
        "model = Model(layers=2, batches=1, rank=0, dist=Dist(1))\n",
        "weight, _ = model.load_weights(0, shard=0, total=4)\n",
        "weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2597b152",
      "metadata": {
        "id": "2597b152"
      },
      "outputs": [],
      "source": [
        "# Combine togegher two shards on one machine.\n",
        "weights = [model.load_weights(0, shard=i, total=ranks)[0] for i in range(ranks)]\n",
        "weights[0].combine(weights[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3d10b6",
      "metadata": {
        "id": "5e3d10b6"
      },
      "outputs": [],
      "source": [
        "# Use allgather to collect the shards from all machines.\n",
        "async def mygather(model: Model) -> WeightGrad:\n",
        "    # Allreduce sums together all the weight grads\n",
        "    return await model.allgather(weights[model.rank], 0)\n",
        "\n",
        "dist = Dist(ranks)\n",
        "out_weights = await asyncio.gather(*[\n",
        "    mygather(Model(layers=1, batches=1, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "out_weights[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3903613",
      "metadata": {
        "id": "c3903613"
      },
      "source": [
        "### Puzzle 3: Weight-Sharded Data Parallel\n",
        "\n",
        "Run a model that shards each layer weight over all the machines. Reconstruct the layer weight at each layer using allgather. Finally update the weights on each machine using allreduce.\n",
        "\n",
        "* Total Steps: 19.9\n",
        "* Total Memory: 2752512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b674be7",
      "metadata": {
        "id": "4b674be7"
      },
      "outputs": [],
      "source": [
        "async def wsdp(model: Model) -> Model:\n",
        "    # Storage on device.\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "\n",
        "    # Load all the activations\n",
        "    model.activations[0] = model.get_activation([model.rank])\n",
        "\n",
        "    # Load a shard of the weights for every layer. Load in the full weights\n",
        "    for l in range(model.LAYERS):\n",
        "        weights[l], opt_states[l] = model.load_weights(l, model.rank, model.RANKS)\n",
        "\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in range(model.LAYERS):\n",
        "        model.set_final_weight(l, weights[l])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c23a1eb",
      "metadata": {
        "id": "6c23a1eb"
      },
      "outputs": [],
      "source": [
        "dist = Dist(ranks)\n",
        "out = await asyncio.gather(*[\n",
        "    wsdp(Model(layers=6, batches=ranks, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "draw_group(out[1].final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09731c67",
      "metadata": {
        "id": "09731c67"
      },
      "outputs": [],
      "source": [
        "draw(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ff46b6",
      "metadata": {
        "id": "d3ff46b6"
      },
      "outputs": [],
      "source": [
        "Model.check(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32243386",
      "metadata": {
        "id": "32243386"
      },
      "source": [
        "## Communication: Scatter-Reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44610031",
      "metadata": {
        "id": "44610031"
      },
      "source": [
        "Scatter across shards\n",
        "Reduce across batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1034dbb9",
      "metadata": {
        "id": "1034dbb9"
      },
      "outputs": [],
      "source": [
        "grad_weight = WeightGrad(0, 1, batches={1}, total_batches=4,\n",
        "                         shards={1}, total=4)\n",
        "grad_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02680e7e",
      "metadata": {
        "id": "02680e7e"
      },
      "outputs": [],
      "source": [
        "grad_weights = {i: WeightGrad(0, 1, batches={i}, total_batches=4,\n",
        "                         shards={0,1,2,3}, total=4) for i in range(4)}\n",
        "grad_weights[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1498773a",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "1498773a"
      },
      "outputs": [],
      "source": [
        "async def scatterreduce(model: Model) -> WeightGrad:\n",
        "    # Allreduce sums together all the weight grads\n",
        "    return await model.scatterreduce(grad_weights[model.rank], 0)\n",
        "\n",
        "dist = Dist(ranks)\n",
        "out = await asyncio.gather(*[\n",
        "    scatterreduce(Model(layers=1, batches=1, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "out[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261435f2",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "261435f2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7e62da15",
      "metadata": {
        "id": "7e62da15"
      },
      "source": [
        "### Puzzle 4: Fully-Sharded Data Parallel\n",
        "\n",
        "Run a model that shards each layer weight over all the machines. Reconstruct the layer weight at each layer using allgather. Collect the gradients with scatter-reduce.\n",
        "\n",
        "* Total Steps: 19.9\n",
        "* Total Memory: 2359296"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a35535",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "43a35535"
      },
      "outputs": [],
      "source": [
        "async def fsdp(model: Model) -> Model:\n",
        "    # Storage on device.\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "\n",
        "    # Load all the activations\n",
        "    model.activations[0] = model.get_activation([model.rank])\n",
        "\n",
        "    # Load a shard of the weights for every layer. Load in the full weights\n",
        "    for l in range(model.LAYERS):\n",
        "        weights[l], opt_states[l] = model.load_weights(l, model.rank, model.RANKS)\n",
        "\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in range(model.LAYERS):\n",
        "        model.set_final_weight(l, weights[l])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec61bda",
      "metadata": {
        "id": "dec61bda"
      },
      "outputs": [],
      "source": [
        "dist = Dist(ranks)\n",
        "out = await asyncio.gather(*[\n",
        "    fsdp(Model(layers=6, batches=ranks, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "draw_group(out[1].final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f62f28",
      "metadata": {
        "id": "b9f62f28"
      },
      "outputs": [],
      "source": [
        "draw(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a527fc7",
      "metadata": {
        "id": "3a527fc7"
      },
      "outputs": [],
      "source": [
        "Model.check(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0858bb4f",
      "metadata": {
        "id": "0858bb4f"
      },
      "source": [
        "## Communication: Point-to-Point\n",
        "\n",
        "An alternative approach to communication is to directly communicate specific information between GPUs. In our model, both GPUs talking to each other block and wait for the handoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de38df4d",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "de38df4d"
      },
      "outputs": [],
      "source": [
        "async def talk(model: Model) -> None:\n",
        "    if model.rank == 0:\n",
        "        await model.pass_to(1, \"extra cheese\")\n",
        "        val = await model.receive()\n",
        "        print(val)\n",
        "    else:\n",
        "        val = await model.receive()\n",
        "        print(val)\n",
        "        val = await model.pass_to(0, \"pizza\")\n",
        "\n",
        "dist = Dist(2)\n",
        "result = await asyncio.gather(*[\n",
        "    talk(Model(layers=1, batches=1, rank=i, dist=dist))\n",
        "    for i in range(2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027b159c",
      "metadata": {
        "id": "027b159c"
      },
      "source": [
        "### Puzzle 5: Pipeline Parallelism\n",
        "\n",
        "Split the layer weights and optimizers equally between GPUs. Have each GPU handle only its layer. Pass the full set of batches for activations and grad_activations between layers using p2p communication. No need for any global communication.\n",
        "\n",
        "* Total Steps: 66.9\n",
        "* Total Memory: 3145728"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09feb2a6",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "09feb2a6"
      },
      "outputs": [],
      "source": [
        "async def pipeline(model: Model) -> Model:\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "    per_rank = model.LAYERS // model.RANKS\n",
        "    my_layers = list([l + (model.rank * per_rank) for l in range(per_rank)])\n",
        "    for l in my_layers:\n",
        "        weights[l], opt_states[l] = model.load_weights(l)\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in my_layers:\n",
        "        model.set_final_weight(l, weights[l])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5c381b",
      "metadata": {
        "id": "2e5c381b"
      },
      "outputs": [],
      "source": [
        "dist = Dist(ranks)\n",
        "out = await asyncio.gather(*[\n",
        "    pipeline(Model(layers=8, batches=ranks, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "draw_group(out[1].final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a99ecad",
      "metadata": {
        "id": "5a99ecad"
      },
      "outputs": [],
      "source": [
        "draw(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2f11d5",
      "metadata": {
        "id": "2b2f11d5"
      },
      "outputs": [],
      "source": [
        "Model.check(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6606efc",
      "metadata": {
        "id": "f6606efc"
      },
      "source": [
        "### Puzzle 6: GPipe Schedule\n",
        "\n",
        "A major issue with the pipeline approach is that it causes a \"bubble\", i.e. time in the later layers waiting for the earlier layers to complete. An alternative approach is to split the batches smaller so you can pass them earlier.\n",
        "\n",
        "In this puzzle, you should run each batch by itself, and then pass. The graph should look similar as the one above but with a smaller bubble.\n",
        "\n",
        "* Total Steps: 33.3\n",
        "* Total Memory: 4194304"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f33513",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "f5f33513"
      },
      "outputs": [],
      "source": [
        "async def gpipe(model: Model) -> Model:\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "    per_rank = model.LAYERS // model.RANKS\n",
        "    my_layers = list([l + (model.rank * per_rank) for l in range(per_rank)])\n",
        "    for l in my_layers:\n",
        "        weights[l], opt_states[l] = model.load_weights(l)\n",
        "\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in my_layers:\n",
        "        model.set_final_weight(l, weights[l])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c73b29",
      "metadata": {
        "id": "f5c73b29"
      },
      "outputs": [],
      "source": [
        "dist = Dist(ranks)\n",
        "out = await asyncio.gather(*[\n",
        "    gpipe(Model(layers=8, batches=ranks, rank=i, dist=dist))\n",
        "    for i in range(ranks)])\n",
        "draw_group(out[1].final_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e759da9",
      "metadata": {
        "id": "0e759da9"
      },
      "outputs": [],
      "source": [
        "draw(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d3102b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "47d3102b"
      },
      "outputs": [],
      "source": [
        "Model.check(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4bc062",
      "metadata": {
        "id": "3a4bc062"
      },
      "source": [
        "### Puzzle 7: Pipeline + FSDP\n",
        "\n",
        "As a last exercise, we can put everything together. Here we are going to run a combination of pipeline parallelism while also sharding our weight between 16 different machines. Here the model only has 4 layers, so we will assign 4 GPUs to each layer in the pipeline parallel approach.\n",
        "\n",
        "This example requires combining both collective communication and p2p communication effectively.\n",
        "\n",
        "* Total Steps: 15.5\n",
        "* Total Memory: 966656"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34757c26",
      "metadata": {
        "id": "34757c26"
      },
      "outputs": [],
      "source": [
        "async def pipeline_fsdp(model: Model) -> Model:\n",
        "    weights, opt_states, activations, grad_activations, grad_weights = model.storage()\n",
        "    per_rank = model.LAYERS // (model.RANKS // 4)\n",
        "    my_layers = list([l + ((model.rank % 4)  * per_rank) for l in range(per_rank)])\n",
        "    for l in range(model.LAYERS):\n",
        "        weights[l, 0], opt_states[l, 0] = model.load_weights(l, model.rank, model.RANKS)\n",
        "    def empty_grad(l):\n",
        "        return model.fake_grad(l, [])\n",
        "    assert False, 'TODO: Implement me'\n",
        "    for l in range(model.LAYERS):\n",
        "        model.set_final_weight(l, weights[l])\n",
        "    # Update\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a0877b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "35a0877b"
      },
      "outputs": [],
      "source": [
        "dist = Dist(16)\n",
        "out = await asyncio.gather(*[\n",
        "    pipeline_fsdp(Model(layers=4, batches=ranks, rank=i, dist=dist))\n",
        "    for i in range(16)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d93c6256",
      "metadata": {
        "id": "d93c6256"
      },
      "outputs": [],
      "source": [
        "Model.check(out)\n",
        "chalk.set_svg_height(1000)\n",
        "chalk.set_svg_draw_height(1000)\n",
        "\n",
        "draw(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24393483",
      "metadata": {
        "id": "24393483"
      },
      "source": [
        "### When does it make sense to combine?\n",
        "\n",
        "The goal of these exercises is to give you a sense of the different methods out there for distributed training. However, there is not currently a one size fits all approach for distributed training. The right choice will depend on the constants such as batch size, memory per GPU, communication overhead, implementation complexity, model size, and specifics of architecture.\n",
        "\n",
        "As an example  of what's left to explore, this last method Pipeline + FSDP is often not a great choice due to the complexities of communication speed. And in fact GPipe + FSDP also gets you into a bad place. The paper [Breadth First Pipeline Parallelism](https://arxiv.org/pdf/2211.05953.pdf) proposes instead a combination of pipeline scheduling and communication. Here's what it looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c9493a",
      "metadata": {
        "id": "35c9493a"
      },
      "source": [
        "![image.png](https://github.com/srush/LLM-Training-Puzzles/assets/35882/f286089a-83bd-483c-b441-f154821d161c)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "custom_cell_magics": "kql"
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}